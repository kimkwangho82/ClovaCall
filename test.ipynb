{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_complex.tensor import ComplexTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 5) [[0.4359949  0.02592623 0.54966248 0.43532239 0.4203678 ]\n",
      " [0.33033482 0.20464863 0.61927097 0.29965467 0.26682728]\n",
      " [0.62113383 0.52914209 0.13457995 0.51357812 0.18443987]\n",
      " [0.78533515 0.85397529 0.49423684 0.84656149 0.07964548]\n",
      " [0.50524609 0.0652865  0.42812233 0.09653092 0.12715997]\n",
      " [0.59674531 0.226012   0.10694568 0.22030621 0.34982629]\n",
      " [0.46778748 0.20174323 0.64040673 0.48306984 0.50523672]\n",
      " [0.38689265 0.79363745 0.58000418 0.1622986  0.70075235]\n",
      " [0.96455108 0.50000836 0.88952006 0.34161365 0.56714413]\n",
      " [0.42754596 0.43674726 0.77655918 0.53560417 0.95374223]\n",
      " [0.54420816 0.08209492 0.3663424  0.8508505  0.40627504]]\n",
      "std :  (11, 1) [[0.16144169]\n",
      " [0.11323727]\n",
      " [0.16331676]\n",
      " [0.23071654]\n",
      " [0.1616835 ]\n",
      " [0.1384343 ]\n",
      " [0.11459825]\n",
      " [0.17431257]\n",
      " [0.16045515]\n",
      " [0.13854918]\n",
      " [0.19526545]]\n",
      "mean :  (11, 1) [[0.30755925]\n",
      " [0.29044176]\n",
      " [0.32356832]\n",
      " [0.45770237]\n",
      " [0.20808267]\n",
      " [0.25448326]\n",
      " [0.37310758]\n",
      " [0.41004184]\n",
      " [0.49203707]\n",
      " [0.47833751]\n",
      " [0.35642744]]\n",
      "std :  (11, 1) [[0.16144169]\n",
      " [0.11323727]\n",
      " [0.16331676]\n",
      " [0.23071654]\n",
      " [0.1616835 ]\n",
      " [0.1384343 ]\n",
      " [0.11459825]\n",
      " [0.17431257]\n",
      " [0.16045515]\n",
      " [0.13854918]\n",
      " [0.19526545]]\n",
      "x :  (11, 5) [[ 0.3363361  -1.74653402  0.80820448  0.33343454  0.26855889]\n",
      " [-0.0442531  -0.92066704  1.69144199 -0.25029888 -0.47622298]\n",
      " [ 0.9769817   0.61927839 -1.20811746  0.55663698 -0.9447796 ]\n",
      " [ 0.52837037  0.69188698 -0.2430982   0.67451983 -1.65167897]\n",
      " [ 1.24238858 -0.89581741  0.91708713 -0.71702549 -0.5466328 ]\n",
      " [ 1.54213311 -0.3663589  -1.10434104 -0.40005582  0.32862264]\n",
      " [ 0.09292094 -1.65215774  1.06316315  0.18330625  0.3127674 ]\n",
      " [-0.47601902  0.99937611  0.27184301 -1.48952108  0.69432097]\n",
      " [ 1.14191844 -0.53950521  0.8992282  -1.23501156 -0.26662986]\n",
      " [-0.88330115 -0.83692879  0.69535555 -0.35665031  1.3815247 ]\n",
      " [ 0.39988548 -1.42128849 -0.22681971  1.32751504 -0.07929231]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "spect = np.random.rand(11, 5)\n",
    "# spect = np.uniform()\n",
    "print(spect.shape, spect)\n",
    "\n",
    "spect = np.log1p(spect)\n",
    "\n",
    "x = spect\n",
    "# x = spect.T\n",
    "\n",
    "mean = np.mean(x, axis=1, keepdims=True)\n",
    "std = np.std(x, axis=1, ddof=1, keepdims=True)\n",
    "print(\"std : \", std.shape, std)\n",
    "std = np.clip(std, a_min=1e-20, a_max=None)\n",
    "print(\"mean : \", mean.shape, mean)\n",
    "print(\"std : \", std.shape, std)\n",
    "\n",
    "x -= mean\n",
    "x = x / std\n",
    "print(\"x : \", x.shape, x)\n",
    "\n",
    "# x = x.T\n",
    "# print(\"x : \", x.shape, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 5) [[0.4359949  0.02592623 0.54966248 0.43532239 0.4203678 ]\n",
      " [0.33033482 0.20464863 0.61927097 0.29965467 0.26682728]\n",
      " [0.62113383 0.52914209 0.13457995 0.51357812 0.18443987]\n",
      " [0.78533515 0.85397529 0.49423684 0.84656149 0.07964548]\n",
      " [0.50524609 0.0652865  0.42812233 0.09653092 0.12715997]\n",
      " [0.59674531 0.226012   0.10694568 0.22030621 0.34982629]\n",
      " [0.46778748 0.20174323 0.64040673 0.48306984 0.50523672]\n",
      " [0.38689265 0.79363745 0.58000418 0.1622986  0.70075235]\n",
      " [0.96455108 0.50000836 0.88952006 0.34161365 0.56714413]\n",
      " [0.42754596 0.43674726 0.77655918 0.53560417 0.95374223]\n",
      " [0.54420816 0.08209492 0.3663424  0.8508505  0.40627504]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "spect = np.random.rand(11, 5)\n",
    "# spect = np.uniform()\n",
    "print(spect.shape, spect)\n",
    "\n",
    "# spect = np.log1p(spect)\n",
    "# print(spect.shape, spect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Argument #4: Padding size should be less than the corresponding input dimension, but got: padding (160, 160) at dimension 2 of input [1, 11, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7b8f941ed4c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mstft_conf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# D.shape = (Freq, Frames, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# D.shape = (Frames, Freq, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lecture/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mstft\u001b[0;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mextended_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msignal_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mpad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_fft\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextended_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msignal_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monesided\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lecture/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   2740\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'3D tensors expect 2 values for padding'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2741\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'reflect'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2742\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreflection_pad1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2743\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'replicate'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2744\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplication_pad1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Argument #4: Padding size should be less than the corresponding input dimension, but got: padding (160, 160) at dimension 2 of input [1, 11, 5]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "y = spect\n",
    "\n",
    "stft_conf = dict(\n",
    "    n_fft = 320,\n",
    "    win_length = 320,\n",
    "    hop_length = 160,\n",
    "    center = True,\n",
    "    window = torch.hamming_window(window_length=320,\n",
    "                                  dtype=torch.float32,\n",
    "                                  device=torch.device('cpu')),\n",
    "    normalized = False,\n",
    "    onesided = True\n",
    ")\n",
    "\n",
    "y = torch.from_numpy(y)\n",
    "print(y.size())\n",
    "D = torch.stft(y, **stft_conf) # D.shape = (Freq, Frames, 2)\n",
    "D = D.transpose(0, 1) # D.shape = (Frames, Freq, 2)\n",
    "\n",
    "# 2. STFT --> Power Spectrum\n",
    "input_stft = D\n",
    "input_stft = ComplexTensor(input_stft[..., 0], input_stft[..., 1])\n",
    "\n",
    "input_power = (input_stft.real ** 2) + (input_stft.imag ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "outputs_channel = 32\n",
    "\n",
    "seq = nn.Sequential(\n",
    "            nn.Conv2d(1, outputs_channel, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5)),\n",
    "            nn.BatchNorm2d(outputs_channel),\n",
    "            nn.Hardtanh(0, 20, inplace=True),\n",
    "            nn.Conv2d(outputs_channel, outputs_channel, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5)),\n",
    "            nn.BatchNorm2d(outputs_channel),\n",
    "            nn.Hardtanh(0, 20, inplace=True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))\n",
      "1 BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "2 Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      "3 Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
      "4 BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "5 Hardtanh(min_val=0, max_val=20, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "for i, module in enumerate(seq):\n",
    "    print(i, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.modules of Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))\n",
      "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      "  (3): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
      "  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(seq.modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(2, 4, batch_first=True)\n",
      "('weight_ih_l0', Parameter containing:\n",
      "tensor([[-0.2919,  0.4298],\n",
      "        [ 0.2231,  0.2423],\n",
      "        [ 0.0263, -0.2563],\n",
      "        [ 0.0846, -0.4668],\n",
      "        [-0.3613, -0.2578],\n",
      "        [ 0.3155,  0.2932],\n",
      "        [-0.2217, -0.0180],\n",
      "        [ 0.3198,  0.4971],\n",
      "        [ 0.1984,  0.0675],\n",
      "        [ 0.3352, -0.2944],\n",
      "        [ 0.0932, -0.3877],\n",
      "        [-0.3465, -0.2583],\n",
      "        [ 0.2262,  0.2011],\n",
      "        [-0.2962,  0.1511],\n",
      "        [ 0.2745, -0.0631],\n",
      "        [ 0.0191,  0.1159]], requires_grad=True))\n",
      "('weight_hh_l0', Parameter containing:\n",
      "tensor([[ 0.3102,  0.4801, -0.3853, -0.1832],\n",
      "        [ 0.1965,  0.4143,  0.4351,  0.4412],\n",
      "        [ 0.0995, -0.4348,  0.0460, -0.3128],\n",
      "        [-0.4660,  0.4442,  0.3802, -0.4988],\n",
      "        [ 0.0936, -0.0842, -0.0823, -0.2289],\n",
      "        [ 0.1923, -0.2962,  0.1833,  0.2529],\n",
      "        [ 0.3579,  0.1870, -0.4949, -0.3243],\n",
      "        [ 0.2497,  0.1047, -0.3900, -0.2879],\n",
      "        [ 0.4704,  0.3369, -0.2180, -0.1258],\n",
      "        [-0.4763, -0.0090, -0.3765, -0.3857],\n",
      "        [-0.0275,  0.0751, -0.2048,  0.2967],\n",
      "        [-0.3043,  0.4537,  0.3426, -0.4216],\n",
      "        [-0.1244,  0.0226,  0.0730,  0.1186],\n",
      "        [ 0.1962,  0.0300, -0.2440,  0.2366],\n",
      "        [-0.4796, -0.2964, -0.1252, -0.2436],\n",
      "        [-0.1749, -0.4098, -0.1064,  0.1069]], requires_grad=True))\n",
      "('bias_ih_l0', Parameter containing:\n",
      "tensor([-0.3257, -0.0257,  0.3579, -0.0514,  0.0139, -0.0431,  0.1012,  0.3179,\n",
      "         0.4736,  0.3175,  0.4747, -0.0362, -0.4492, -0.2370,  0.3405, -0.0032],\n",
      "       requires_grad=True))\n",
      "('bias_hh_l0', Parameter containing:\n",
      "tensor([-0.2485, -0.3832, -0.4679, -0.4220, -0.1014,  0.2742,  0.2703, -0.4822,\n",
      "         0.3119, -0.3913, -0.1057, -0.2027, -0.0963, -0.0982, -0.4487, -0.4317],\n",
      "       requires_grad=True))\n",
      "True\n",
      "lstm:  PackedSequence(data=tensor([[ 0.0671, -0.0045,  0.1071, -0.0065],\n",
      "        [ 0.1658, -0.1523, -0.0542, -0.0481],\n",
      "        [ 0.0753,  0.0514,  0.2734, -0.0075],\n",
      "        [ 0.1506,  0.0072,  0.1049, -0.1466],\n",
      "        [ 0.1179,  0.0342,  0.2262, -0.0454],\n",
      "        [ 0.0985,  0.0560,  0.2595, -0.0397],\n",
      "        [ 0.1184, -0.0045,  0.2090, -0.0108],\n",
      "        [ 0.1182,  0.0124,  0.2145, -0.0058],\n",
      "        [ 0.1256, -0.0277,  0.2130, -0.0044],\n",
      "        [ 0.1044,  0.0374,  0.3227, -0.0054]], grad_fn=<CatBackward>), batch_sizes=tensor([2, 2, 2, 2, 1, 1]), sorted_indices=tensor([1, 0]), unsorted_indices=tensor([1, 0]))\n",
      "h   :  torch.Size([1, 2, 4])\n",
      "pad :  torch.Size([2, 6, 4]) tensor([[[ 0.1658, -0.1523, -0.0542, -0.0481],\n",
      "         [ 0.1506,  0.0072,  0.1049, -0.1466],\n",
      "         [ 0.0985,  0.0560,  0.2595, -0.0397],\n",
      "         [ 0.1182,  0.0124,  0.2145, -0.0058],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0671, -0.0045,  0.1071, -0.0065],\n",
      "         [ 0.0753,  0.0514,  0.2734, -0.0075],\n",
      "         [ 0.1179,  0.0342,  0.2262, -0.0454],\n",
      "         [ 0.1184, -0.0045,  0.2090, -0.0108],\n",
      "         [ 0.1256, -0.0277,  0.2130, -0.0044],\n",
      "         [ 0.1044,  0.0374,  0.3227, -0.0054]]], grad_fn=<IndexSelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "random_seed = 0\n",
    "\n",
    "# torch.manual_seed(random_seed)\n",
    "# torch.cuda.manual_seed(random_seed)\n",
    "# torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# np.random.seed(random_seed)\n",
    "# random.seed(random_seed)\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "B = 2\n",
    "T = 6\n",
    "V = 10\n",
    "E = 2\n",
    "H = 4\n",
    "\n",
    "\n",
    "embedding_layer = nn.Embedding(V, E)\n",
    "lstm = nn.LSTM(E, H, 1, batch_first=True)\n",
    "print(lstm)\n",
    "\n",
    "# lstm2 = nn.LSTM(E, H, 1)\n",
    "# lstm2 = lstm2.load_state_dict(lstm.state_dict())\n",
    "# print(lstm2)\n",
    "\n",
    "# lstm2 = copy.deepcopy(lstm)\n",
    "# lstm2.batch_first = False\n",
    "# print(lstm2)\n",
    "\n",
    "# lstm2 = nn.LSTM(E, H, 1, batch_first=False)\n",
    "# print(lstm2)\n",
    "\n",
    "\n",
    "for name_1 in lstm.named_parameters():\n",
    "    print(name_1)\n",
    "\n",
    "\n",
    "x = torch.LongTensor([[3, 9, 2, 1, 0, 0],\n",
    "                      [1, 2, 4, 1, 1, 2]])\n",
    "\n",
    "input_lengths = torch.LongTensor([4, 6])\n",
    "\n",
    "embed = embedding_layer(x)\n",
    "\n",
    "\n",
    "\n",
    "packed = pack_padded_sequence(embed, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "# print(\"pack:\", packed)\n",
    "# print(packed.data.size())\n",
    "\n",
    "output, (h, c) = lstm(packed)\n",
    "print(lstm.batch_first)\n",
    "print(\"lstm: \", output)\n",
    "print(\"h   : \", h.size())\n",
    "\n",
    "output, output_lengths = pad_packed_sequence(output, batch_first=True)\n",
    "print(\"pad : \", output.size(), output)\n",
    "\n",
    "# print(\"#\"*100)\n",
    "\n",
    "# packed = pack_padded_sequence(embed, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "# #print(\"pack:\", packed)\n",
    "# # print(packed.data.size())\n",
    "\n",
    "# output, (h, c) = lstm2(packed)\n",
    "# print(lstm2.batch_first)\n",
    "# print(\"lstm: \", output)\n",
    "# print(\"h   : \", h.size())\n",
    "\n",
    "# output, output_lengths = pad_packed_sequence(output, batch_first=True)\n",
    "# print(\"pad : \", output.size(), output)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(2, 4)\n",
      "('weight_ih_l0', Parameter containing:\n",
      "tensor([[-0.2919,  0.4298],\n",
      "        [ 0.2231,  0.2423],\n",
      "        [ 0.0263, -0.2563],\n",
      "        [ 0.0846, -0.4668],\n",
      "        [-0.3613, -0.2578],\n",
      "        [ 0.3155,  0.2932],\n",
      "        [-0.2217, -0.0180],\n",
      "        [ 0.3198,  0.4971],\n",
      "        [ 0.1984,  0.0675],\n",
      "        [ 0.3352, -0.2944],\n",
      "        [ 0.0932, -0.3877],\n",
      "        [-0.3465, -0.2583],\n",
      "        [ 0.2262,  0.2011],\n",
      "        [-0.2962,  0.1511],\n",
      "        [ 0.2745, -0.0631],\n",
      "        [ 0.0191,  0.1159]], requires_grad=True))\n",
      "('weight_hh_l0', Parameter containing:\n",
      "tensor([[ 0.3102,  0.4801, -0.3853, -0.1832],\n",
      "        [ 0.1965,  0.4143,  0.4351,  0.4412],\n",
      "        [ 0.0995, -0.4348,  0.0460, -0.3128],\n",
      "        [-0.4660,  0.4442,  0.3802, -0.4988],\n",
      "        [ 0.0936, -0.0842, -0.0823, -0.2289],\n",
      "        [ 0.1923, -0.2962,  0.1833,  0.2529],\n",
      "        [ 0.3579,  0.1870, -0.4949, -0.3243],\n",
      "        [ 0.2497,  0.1047, -0.3900, -0.2879],\n",
      "        [ 0.4704,  0.3369, -0.2180, -0.1258],\n",
      "        [-0.4763, -0.0090, -0.3765, -0.3857],\n",
      "        [-0.0275,  0.0751, -0.2048,  0.2967],\n",
      "        [-0.3043,  0.4537,  0.3426, -0.4216],\n",
      "        [-0.1244,  0.0226,  0.0730,  0.1186],\n",
      "        [ 0.1962,  0.0300, -0.2440,  0.2366],\n",
      "        [-0.4796, -0.2964, -0.1252, -0.2436],\n",
      "        [-0.1749, -0.4098, -0.1064,  0.1069]], requires_grad=True))\n",
      "('bias_ih_l0', Parameter containing:\n",
      "tensor([-0.3257, -0.0257,  0.3579, -0.0514,  0.0139, -0.0431,  0.1012,  0.3179,\n",
      "         0.4736,  0.3175,  0.4747, -0.0362, -0.4492, -0.2370,  0.3405, -0.0032],\n",
      "       requires_grad=True))\n",
      "('bias_hh_l0', Parameter containing:\n",
      "tensor([-0.2485, -0.3832, -0.4679, -0.4220, -0.1014,  0.2742,  0.2703, -0.4822,\n",
      "         0.3119, -0.3913, -0.1057, -0.2027, -0.0963, -0.0982, -0.4487, -0.4317],\n",
      "       requires_grad=True))\n",
      "False\n",
      "lstm:  PackedSequence(data=tensor([[ 0.0671, -0.0045,  0.1071, -0.0065],\n",
      "        [ 0.1658, -0.1523, -0.0542, -0.0481],\n",
      "        [ 0.0753,  0.0514,  0.2734, -0.0075],\n",
      "        [ 0.1506,  0.0072,  0.1049, -0.1466],\n",
      "        [ 0.1179,  0.0342,  0.2262, -0.0454],\n",
      "        [ 0.0985,  0.0560,  0.2595, -0.0397],\n",
      "        [ 0.1184, -0.0045,  0.2090, -0.0108],\n",
      "        [ 0.1182,  0.0124,  0.2145, -0.0058],\n",
      "        [ 0.1256, -0.0277,  0.2130, -0.0044],\n",
      "        [ 0.1044,  0.0374,  0.3227, -0.0054]], grad_fn=<CatBackward>), batch_sizes=tensor([2, 2, 2, 2, 1, 1]), sorted_indices=tensor([1, 0]), unsorted_indices=tensor([1, 0]))\n",
      "h   :  torch.Size([1, 2, 4])\n",
      "pad :  torch.Size([2, 6, 4]) tensor([[[ 0.1658, -0.1523, -0.0542, -0.0481],\n",
      "         [ 0.1506,  0.0072,  0.1049, -0.1466],\n",
      "         [ 0.0985,  0.0560,  0.2595, -0.0397],\n",
      "         [ 0.1182,  0.0124,  0.2145, -0.0058],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0671, -0.0045,  0.1071, -0.0065],\n",
      "         [ 0.0753,  0.0514,  0.2734, -0.0075],\n",
      "         [ 0.1179,  0.0342,  0.2262, -0.0454],\n",
      "         [ 0.1184, -0.0045,  0.2090, -0.0108],\n",
      "         [ 0.1256, -0.0277,  0.2130, -0.0044],\n",
      "         [ 0.1044,  0.0374,  0.3227, -0.0054]]], grad_fn=<IndexSelectBackward>)\n",
      "output_lengths :  tensor([4, 6])\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "random_seed = 0\n",
    "\n",
    "# torch.manual_seed(random_seed)\n",
    "# torch.cuda.manual_seed(random_seed)\n",
    "# torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# np.random.seed(random_seed)\n",
    "# random.seed(random_seed)\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "B = 2\n",
    "T = 6\n",
    "V = 10\n",
    "E = 2\n",
    "H = 4\n",
    "\n",
    "\n",
    "embedding_layer = nn.Embedding(V, E)\n",
    "lstm = nn.LSTM(E, H, 1, batch_first=False)\n",
    "print(lstm)\n",
    "\n",
    "# lstm2 = nn.LSTM(E, H, 1)\n",
    "# lstm2 = lstm2.load_state_dict(lstm.state_dict())\n",
    "# print(lstm2)\n",
    "\n",
    "# lstm2 = copy.deepcopy(lstm)\n",
    "# lstm2.batch_first = False\n",
    "# print(lstm2)\n",
    "\n",
    "# lstm2 = nn.LSTM(E, H, 1, batch_first=False)\n",
    "# print(lstm2)\n",
    "\n",
    "\n",
    "for name_1 in lstm.named_parameters():\n",
    "    print(name_1)\n",
    "\n",
    "# for name_1, name_2 in zip(lstm.named_parameters(), lstm2.named_parameters()):\n",
    "#     print(name_1, name_2)\n",
    "\n",
    "\n",
    "x = torch.LongTensor([[3, 9, 2, 1, 0, 0],\n",
    "                      [1, 2, 4, 1, 1, 2]])\n",
    "\n",
    "input_lengths = torch.LongTensor([4, 6])\n",
    "\n",
    "embed = embedding_layer(x)\n",
    "\n",
    "\n",
    "\n",
    "# packed = pack_padded_sequence(embed, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "# # print(\"pack:\", packed)\n",
    "# # print(packed.data.size())\n",
    "\n",
    "# output, (h, c) = lstm(packed)\n",
    "# print(lstm.batch_first)\n",
    "# print(\"lstm: \", output)\n",
    "# print(\"h   : \", h.size())\n",
    "\n",
    "# output, output_lengths = pad_packed_sequence(output, batch_first=True)\n",
    "# print(\"pad : \", output.size(), output)\n",
    "\n",
    "# print(\"#\"*100)\n",
    "\n",
    "packed = pack_padded_sequence(embed, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "#print(\"pack:\", packed)\n",
    "# print(packed.data.size())\n",
    "\n",
    "output, (h, c) = lstm(packed)\n",
    "print(lstm.batch_first)\n",
    "print(\"lstm: \", output)\n",
    "print(\"h   : \", h.size())\n",
    "\n",
    "output, output_lengths = pad_packed_sequence(output, batch_first=True)\n",
    "print(\"pad : \", output.size(), output)\n",
    "print(\"output_lengths : \", output_lengths)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h   :  torch.Size([1, 2, 4])\n",
    "pad :  torch.Size([2, 6, 4]) tensor([[[ 0.1658, -0.1523, -0.0542, -0.0481],\n",
    "         [ 0.1506,  0.0072,  0.1049, -0.1466],\n",
    "         [ 0.0985,  0.0560,  0.2595, -0.0397],\n",
    "         [ 0.1182,  0.0124,  0.2145, -0.0058],\n",
    "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
    "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
    "\n",
    "        [[ 0.0671, -0.0045,  0.1071, -0.0065],\n",
    "         [ 0.0753,  0.0514,  0.2734, -0.0075],\n",
    "         [ 0.1179,  0.0342,  0.2262, -0.0454],\n",
    "         [ 0.1184, -0.0045,  0.2090, -0.0108],\n",
    "         [ 0.1256, -0.0277,  0.2130, -0.0044],\n",
    "         [ 0.1044,  0.0374,  0.3227, -0.0054]]], grad_fn=<IndexSelectBackward>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6)\n",
      "(1, 2, 6)\n"
     ]
    }
   ],
   "source": [
    "x = torch.LongTensor([[3, 9, 2, 1, 0, 0],\n",
    "                      [1, 2, 4, 1, 1, 2]])\n",
    "\n",
    "x_np = x.cpu().numpy()\n",
    "\n",
    "print(x_np.shape)\n",
    "\n",
    "d = x_np[None]\n",
    "print(d.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.defaultdict'>\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "print(defaultdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "sum_dict = defaultdict(lambda: 0)\n",
    "sq_dict = defaultdict(lambda: 0)\n",
    "count_dict = defaultdict(lambda: 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4. 5.]\n",
      "[ 2.  4.  6.  8. 10.]\n"
     ]
    }
   ],
   "source": [
    "d = 0. + np.array([1, 2, 3, 4, 5])\n",
    "print(d)\n",
    "d = d + np.array([1, 2, 3, 4, 5])\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5488135  0.71518937 0.60276338 0.54488318 0.4236548 ]\n",
      " [0.64589411 0.43758721 0.891773   0.96366276 0.38344152]\n",
      " [0.79172504 0.52889492 0.56804456 0.92559664 0.07103606]\n",
      " [0.0871293  0.0202184  0.83261985 0.77815675 0.87001215]\n",
      " [0.97861834 0.79915856 0.46147936 0.78052918 0.11827443]\n",
      " [0.63992102 0.14335329 0.94466892 0.52184832 0.41466194]\n",
      " [0.26455561 0.77423369 0.45615033 0.56843395 0.0187898 ]\n",
      " [0.6176355  0.61209572 0.616934   0.94374808 0.6818203 ]\n",
      " [0.3595079  0.43703195 0.6976312  0.06022547 0.66676672]\n",
      " [0.67063787 0.21038256 0.1289263  0.31542835 0.36371077]\n",
      " [0.57019677 0.43860151 0.98837384 0.10204481 0.20887676]]\n",
      "11\n",
      "[6.17463497 5.11674719 7.18936472 6.50455749 4.22104523]\n",
      "[4.07557701 3.04404961 5.34875057 4.89666502 2.36013627]\n",
      "####################################################################################################\n",
      "11\n",
      "[6.17463497 5.11674719 7.18936472 6.50455749 4.22104523]\n",
      "[4.07557701 3.04404961 5.34875057 4.89666502 2.36013627]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "collect_stats = defaultdict(lambda: 0)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# (Length, Dim) = (11, 5)\n",
    "feat = np.random.rand(11, 5)\n",
    "print(feat)\n",
    "\n",
    "# print(len(feat))\n",
    "# print(feat.sum(0))\n",
    "# print((feat ** 2).sum(0))\n",
    "\n",
    "collect_stats['count'] += len(feat)\n",
    "collect_stats['sum'] += feat.sum(0)\n",
    "collect_stats['sum_square'] += (feat ** 2).sum(0)\n",
    "\n",
    "print(collect_stats['count'])\n",
    "print(collect_stats['sum'])\n",
    "print(collect_stats['sum_square'])\n",
    "\n",
    "np.savez(\"feats_stats.npz\",\n",
    "         count=collect_stats['count'],\n",
    "         sum=collect_stats['sum'],\n",
    "         sum_square=collect_stats['sum_square'])\n",
    "\n",
    "\n",
    "eps: float = 1.0e-20\n",
    "stats_file = \"feats_stats.npz\"\n",
    "stats = np.load(stats_file)\n",
    "\n",
    "count = stats[\"count\"]\n",
    "sum_v = stats[\"sum\"]\n",
    "sum_square_v = stats[\"sum_square\"]\n",
    "mean = sum_v / count\n",
    "var = sum_square_v / count - mean * mean\n",
    "std = np.maximum(np.sqrt(var), eps)\n",
    "\n",
    "print(\"#\"*100)\n",
    "print(count)\n",
    "print(sum_v)\n",
    "print(sum_square_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 3 4 4]\n",
      "[2 2 3 4 5]\n",
      "[2 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "print(np.clip(a, a_min=2, a_max= 4))\n",
    "\n",
    "print(np.maximum(a, 2))\n",
    "print(np.clip(a, a_min=2, a_max=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utterance_mvn\n",
    "\n",
    "(e2e.seq2seq) kwangho@ubuntu_gpu:~/lecture/e2e/seq2seq/ClovaCall$ cat log/LSTM_512x3_512x2_zeroth_korean_trimmed_utterance_mvn/run_las_asr_trainer_CUDA0.sh.log | grep Train\n",
    ">> Train dataset :  data/zeroth_korean/train_zeroth_korean.trimmed.json\n",
    "Train(train) Summary Epoch: [1] Average Loss 2.324      Average CER 92.940      Time 0:23:21.248011\n",
    "Train(train) Summary Epoch: [2] Average Loss 1.898      Average CER 84.313      Time 0:48:26.438334\n",
    "Train(train) Summary Epoch: [3] Average Loss 1.724      Average CER 80.214      Time 1:13:32.644493\n",
    "Train(train) Summary Epoch: [4] Average Loss 1.593      Average CER 76.745      Time 1:38:44.240438\n",
    "Train(train) Summary Epoch: [5] Average Loss 1.482      Average CER 73.035      Time 2:03:51.363183\n",
    "Train(train) Summary Epoch: [6] Average Loss 1.384      Average CER 69.510      Time 2:28:59.029577\n",
    "Train(train) Summary Epoch: [7] Average Loss 1.298      Average CER 65.870      Time 2:54:16.602752\n",
    "Train(train) Summary Epoch: [8] Average Loss 1.223      Average CER 62.722      Time 3:19:35.680655\n",
    "Train(train) Summary Epoch: [9] Average Loss 1.158      Average CER 59.833      Time 3:44:52.772413\n",
    "(e2e.seq2seq) kwangho@ubuntu_gpu:~/lecture/e2e/seq2seq/ClovaCall$ cat log/LSTM_512x3_512x2_zeroth_korean_trimmed_utterance_mvn/run_las_asr_trainer_CUDA0.sh.log | grep Test\n",
    ">> Test dataset :  ['data/zeroth_korean/test_zeroth_korean.trimmed.json']\n",
    "Test(data/zeroth_korean/test_zeroth_korean.trimmed.json) Summary Epoch: [1]     Average Loss 6.455      Average CER 156.792\n",
    "Test(data/zeroth_korean/test_zeroth_korean.trimmed.json) Summary Epoch: [2]     Average Loss 7.706      Average CER 164.226\n",
    "Test(data/zeroth_korean/test_zeroth_korean.trimmed.json) Summary Epoch: [3]     Average Loss 7.822      Average CER 146.557\n",
    "Test(data/zeroth_korean/test_zeroth_korean.trimmed.json) Summary Epoch: [4]     Average Loss 8.531      Average CER 169.235\n",
    "Test(data/zeroth_korean/test_zeroth_korean.trimmed.json) Summary Epoch: [5]     Average Loss 8.584      Average CER 154.148\n",
    "Test(data/zeroth_korean/test_zeroth_korean.trimmed.json) Summary Epoch: [6]     Average Loss 9.207      Average CER 140.357\n",
    "Test(data/zeroth_korean/test_zeroth_korean.trimmed.json) Summary Epoch: [7]     Average Loss 9.219      Average CER 111.287\n",
    "Test(data/zeroth_korean/test_zeroth_korean.trimmed.json) Summary Epoch: [8]     Average Loss 9.430      Average CER 111.816\n",
    "Test(data/zeroth_korean/test_zeroth_korean.trimmed.json) Summary Epoch: [9]     Average Loss 9.803      Average CER 105.195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance\n",
    "\n",
    "(base) kwangho@ubuntu_gpu:~/lecture/e2e/seq2seq/ClovaCall (master) $ cat log/LSTM_512x3_512x2_zeroth_korean_trimmed_instance/run_las_asr_trainer_CUDA1.sh.log\n",
    " | grep Train\n",
    ">> Train dataset :  data/zeroth_korean/train_zeroth_korean.trimmed.json\n",
    "Train(train) Summary Epoch: [1] Average Loss 2.325      Average CER 92.884      Time 0:22:57.441221\n",
    "Train(train) Summary Epoch: [2] Average Loss 1.899      Average CER 84.224      Time 0:47:45.848677\n",
    "Train(train) Summary Epoch: [3] Average Loss 1.726      Average CER 80.213      Time 1:12:33.175360\n",
    "Train(train) Summary Epoch: [4] Average Loss 1.593      Average CER 76.660      Time 1:37:19.637668\n",
    "Train(train) Summary Epoch: [5] Average Loss 1.481      Average CER 72.869      Time 2:01:58.303707\n",
    "Train(train) Summary Epoch: [6] Average Loss 1.383      Average CER 69.265      Time 2:26:57.283479\n",
    "Train(train) Summary Epoch: [7] Average Loss 1.294      Average CER 65.681      Time 2:51:57.533144\n",
    "Train(train) Summary Epoch: [8] Average Loss 1.209      Average CER 62.061      Time 3:16:50.789678\n",
    "Train(train) Summary Epoch: [9] Average Loss 1.136      Average CER 58.900      Time 3:41:37.104262\n",
    "(base) kwangho@ubuntu_gpu:~/lecture/e2e/seq2seq/ClovaCall (master) $ cat log/LSTM_512x3_512x2_zeroth_korean_trimmed_instance/run_las_asr_trainer_CUDA1.sh.log\n",
    " | grep Test\n",
    ">> Test dataset :  ['data/zeroth_korean/test_zeroth_korean.trimmed.json']\n",
    "Test(data/zeroth_korean/test_zeroth_korean.trimmed.json) Summary Epoch: [1]     Average Loss 6.504      Average CER 156.828\n",
    "Test(data/zeroth_korean/test_zeroth_korean.trimmed.json) Summary Epoch: [2]     Average Loss 8.054      Average CER 166.938\n",
    "Test(data/zeroth_korean/test_zeroth_korean.trimmed.json) Summary Epoch: [3]     Average Loss 7.635      Average CER 150.762\n",
    "Test(data/zeroth_korean/test_zeroth_korean.trimmed.json) Summary Epoch: [4]     Average Loss 8.356      Average CER 154.868\n",
    "Test(data/zeroth_korean/test_zeroth_korean.trimmed.json) Summary Epoch: [5]     Average Loss 8.642      Average CER 135.753\n",
    "Test(data/zeroth_korean/test_zeroth_korean.trimmed.json) Summary Epoch: [6]     Average Loss 9.127      Average CER 112.251\n",
    "Test(data/zeroth_korean/test_zeroth_korean.trimmed.json) Summary Epoch: [7]     Average Loss 9.395      Average CER 103.774\n",
    "Test(data/zeroth_korean/test_zeroth_korean.trimmed.json) Summary Epoch: [8]     Average Loss 9.421      Average CER 106.755\n",
    "Test(data/zeroth_korean/test_zeroth_korean.trimmed.json) Summary Epoch: [9]     Average Loss 9.912      Average CER 102.592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-inf]\n",
      "tensor([-inf])\n",
      "0.0\n",
      "tensor([1., 2., -inf]) torch.float32\n",
      "tensor([0.2689, 0.7311, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "d = np.array([-np.inf])\n",
    "print(d)\n",
    "dt = torch.from_numpy(d).float()\n",
    "print(dt)\n",
    "print(np.exp(-np.inf))\n",
    "\n",
    "\n",
    "\n",
    "d = torch.FloatTensor([1, 2, -float('inf')])\n",
    "print(d, d.dtype)\n",
    "\n",
    "\n",
    "r = torch.nn.Softmax(dim=-1)(d)\n",
    "print(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 0],\n",
      "        [3, 0, 0],\n",
      "        [4, 5, 6],\n",
      "        [3, 4, 0]])\n",
      "_emb:  torch.Size([4, 3, 10])\n",
      "PackedSequence(data=tensor([[ 1.1247, -0.1292,  1.9093, -0.5734,  0.3151,  0.7885,  0.0279,  0.3433,\n",
      "          1.3762,  0.8962],\n",
      "        [ 0.5047, -0.4987, -0.4891,  0.0578,  0.9436,  0.9775,  0.7843, -0.1034,\n",
      "          0.1187, -0.2737],\n",
      "        [ 1.0828, -1.7865, -0.7372, -0.7429, -0.4891,  2.4613, -0.7179, -0.2446,\n",
      "          0.6929, -0.3115],\n",
      "        [ 1.0828, -1.7865, -0.7372, -0.7429, -0.4891,  2.4613, -0.7179, -0.2446,\n",
      "          0.6929, -0.3115],\n",
      "        [ 1.2807, -1.8030, -0.4645, -0.6141, -0.6766, -0.3215, -1.0906, -0.7074,\n",
      "          0.2326, -0.0978],\n",
      "        [ 1.1537,  1.3910,  1.0732, -1.2210,  0.9103,  0.8257,  0.1247,  0.8079,\n",
      "          0.1415, -0.5531],\n",
      "        [ 1.1247, -0.1292,  1.9093, -0.5734,  0.3151,  0.7885,  0.0279,  0.3433,\n",
      "          1.3762,  0.8962],\n",
      "        [-0.2917,  0.6454,  2.2931, -0.5244,  1.2735, -3.2211,  0.1115,  0.1109,\n",
      "         -0.7313, -0.3875]], grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([4, 3, 1]), sorted_indices=tensor([2, 0, 3, 1]), unsorted_indices=tensor([1, 3, 0, 2]))\n",
      "hid:  torch.Size([2, 4, 5])\n",
      "torch.Size([4, 3, 10]) tensor([[[ 0.5047, -0.4987, -0.4891,  0.0578,  0.9436,  0.9775,  0.7843,\n",
      "          -0.1034,  0.1187, -0.2737],\n",
      "         [ 1.1537,  1.3910,  1.0732, -1.2210,  0.9103,  0.8257,  0.1247,\n",
      "           0.8079,  0.1415, -0.5531],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 1.0828, -1.7865, -0.7372, -0.7429, -0.4891,  2.4613, -0.7179,\n",
      "          -0.2446,  0.6929, -0.3115],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 1.1247, -0.1292,  1.9093, -0.5734,  0.3151,  0.7885,  0.0279,\n",
      "           0.3433,  1.3762,  0.8962],\n",
      "         [ 1.2807, -1.8030, -0.4645, -0.6141, -0.6766, -0.3215, -1.0906,\n",
      "          -0.7074,  0.2326, -0.0978],\n",
      "         [-0.2917,  0.6454,  2.2931, -0.5244,  1.2735, -3.2211,  0.1115,\n",
      "           0.1109, -0.7313, -0.3875]],\n",
      "\n",
      "        [[ 1.0828, -1.7865, -0.7372, -0.7429, -0.4891,  2.4613, -0.7179,\n",
      "          -0.2446,  0.6929, -0.3115],\n",
      "         [ 1.1247, -0.1292,  1.9093, -0.5734,  0.3151,  0.7885,  0.0279,\n",
      "           0.3433,  1.3762,  0.8962],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000]]], grad_fn=<IndexSelectBackward>)\n",
      "tensor([2, 1, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "seed = 123456\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "emb = torch.nn.Embedding(7, 10)\n",
    "rnn = torch.nn.LSTM(10, 5, 2, batch_first=False, dropout=0.0, bidirectional=False)\n",
    "\n",
    "\n",
    "\n",
    "seq = torch.tensor([[1,2,0], [3,0,0], [4,5,6], [3, 4, 0]])\n",
    "lens = [2, 1, 3, 2]\n",
    "print(seq)\n",
    "\n",
    "\n",
    "_emb = emb(seq)\n",
    "print(\"_emb: \", _emb.size())\n",
    "\n",
    "packed = pack_padded_sequence(_emb, lens, batch_first=True, enforce_sorted=False)\n",
    "print(packed)\n",
    "\n",
    "\n",
    "out, hid = rnn(packed)\n",
    "print(\"hid: \", hid[0].size())\n",
    "\n",
    "\n",
    "\n",
    "seq_unpacked, lens_unpacked = pad_packed_sequence(packed, batch_first=True)\n",
    "print(seq_unpacked.size(), seq_unpacked)\n",
    "print(lens_unpacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_packed_sequence\n",
    ">>> seq = torch.tensor([[1,2,0], [3,0,0], [4,5,6]])\n",
    ">>> lens = [2, 1, 3]\n",
    ">>> packed = pack_padded_sequence(seq, lens, batch_first=True, enforce_sorted=False)\n",
    ">>> packed\n",
    "PackedSequence(data=tensor([4, 1, 3, 5, 2, 6]), batch_sizes=tensor([3, 2, 1]),\n",
    "               sorted_indices=tensor([2, 0, 1]), unsorted_indices=tensor([1, 2, 0]))\n",
    ">>> seq_unpacked, lens_unpacked = pad_packed_sequence(packed, batch_first=True)\n",
    ">>> seq_unpacked\n",
    "tensor([[1, 2, 0],\n",
    "        [3, 0, 0],\n",
    "        [4, 5, 6]])\n",
    ">>> lens_unpacked\n",
    "tensor([2, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1147, 80])\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "import torchaudio\n",
    "\n",
    "y, sr = sf.read('./data/zeroth_korean/train_data_01/003/106/106_003_0077.trimmed.wav')\n",
    "\n",
    "sig = torch.FloatTensor(y).unsqueeze(0)\n",
    "feat = torchaudio.compliance.kaldi.fbank(sig,\n",
    "                                         dither= 0.0,\n",
    "                                         frame_length=25.0,\n",
    "                                         frame_shift=10.0,\n",
    "                                         sample_frequency=16000.0,\n",
    "                                         num_mel_bins=80)\n",
    "print(feat.size())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2]) tensor([[[0.1574, 0.8316],\n",
      "         [0.4780, 0.8731]],\n",
      "\n",
      "        [[0.1574, 0.8316],\n",
      "         [0.4780, 0.8731]],\n",
      "\n",
      "        [[0.1574, 0.8316],\n",
      "         [0.4780, 0.8731]]])\n",
      "torch.Size([3, 2, 2]) tensor([[[0.1574, 0.8316],\n",
      "         [0.4780, 0.8731]],\n",
      "\n",
      "        [[0.1574, 0.8316],\n",
      "         [0.4780, 0.8731]],\n",
      "\n",
      "        [[0.1574, 0.8316],\n",
      "         [0.4780, 0.8731]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "L = 2\n",
    "N = 3\n",
    "A = torch.randn(L,L)\n",
    "\n",
    "\n",
    "D_expand = A.expand(N, L, L) # specifies new size  # view\n",
    "D_repeat = A.repeat(N,1,1) # specifies number of copies # copy\n",
    "\n",
    "print(D_expand.size(), D_expand)\n",
    "print(D_repeat.size(), D_repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function vars>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function vars in module builtins:\n",
      "\n",
      "vars(...)\n",
      "    vars([object]) -> dictionary\n",
      "    \n",
      "    Without arguments, equivalent to locals().\n",
      "    With an argument, equivalent to object.__dict__.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': '__main__',\n",
       " '__doc__': 'Automatically created module for IPython interactive environment',\n",
       " '__package__': None,\n",
       " '__loader__': None,\n",
       " '__spec__': None,\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '_ih': ['',\n",
       "  'import torch\\n\\nL = 10\\nN = 20\\nA = torch.randn(L,L)\\nA.expand(N, L, L) # specifies new size\\nA.repeat(N,1,1) # specifies number of copies',\n",
       "  'import torch\\n\\nL = 2\\nN = 3\\nA = torch.randn(L,L)\\n\\n\\nA.expand(N, L, L) # specifies new size\\nA.repeat(N,1,1) # specifies number of copies',\n",
       "  'import torch\\n\\nL = 2\\nN = 3\\nA = torch.randn(L,L)\\n\\n\\nD_expand = A.expand(N, L, L) # specifies new size\\nD_repeat = A.repeat(N,1,1) # specifies number of copies\\n\\nprint(D_expand.size(), D_expand)',\n",
       "  'import torch\\n\\nL = 2\\nN = 3\\nA = torch.randn(L,L)\\n\\n\\nD_expand = A.expand(N, L, L) # specifies new size\\nD_repeat = A.repeat(N,1,1) # specifies number of copies\\n\\nprint(D_expand.size(), D_expand)\\nprint(D_repeat.size(), D_repeat)',\n",
       "  'vars',\n",
       "  'help(vars)',\n",
       "  'vars()'],\n",
       " '_oh': {1: tensor([[[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "           [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "           [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "           ...,\n",
       "           [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "           [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "           [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       "  \n",
       "          [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "           [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "           [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "           ...,\n",
       "           [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "           [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "           [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       "  \n",
       "          [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "           [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "           [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "           ...,\n",
       "           [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "           [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "           [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "           [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "           [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "           ...,\n",
       "           [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "           [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "           [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       "  \n",
       "          [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "           [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "           [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "           ...,\n",
       "           [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "           [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "           [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       "  \n",
       "          [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "           [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "           [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "           ...,\n",
       "           [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "           [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "           [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]]]),\n",
       "  2: tensor([[[ 1.1109, -1.0908],\n",
       "           [ 0.5226,  1.4406]],\n",
       "  \n",
       "          [[ 1.1109, -1.0908],\n",
       "           [ 0.5226,  1.4406]],\n",
       "  \n",
       "          [[ 1.1109, -1.0908],\n",
       "           [ 0.5226,  1.4406]]]),\n",
       "  5: <function vars>},\n",
       " '_dh': ['/mnt/data-2/INSTALL/GIT/kaldi/egs/zeroth_korean.450h/s5/lecture/e2e/seq2seq/ClovaCall'],\n",
       " 'In': ['',\n",
       "  'import torch\\n\\nL = 10\\nN = 20\\nA = torch.randn(L,L)\\nA.expand(N, L, L) # specifies new size\\nA.repeat(N,1,1) # specifies number of copies',\n",
       "  'import torch\\n\\nL = 2\\nN = 3\\nA = torch.randn(L,L)\\n\\n\\nA.expand(N, L, L) # specifies new size\\nA.repeat(N,1,1) # specifies number of copies',\n",
       "  'import torch\\n\\nL = 2\\nN = 3\\nA = torch.randn(L,L)\\n\\n\\nD_expand = A.expand(N, L, L) # specifies new size\\nD_repeat = A.repeat(N,1,1) # specifies number of copies\\n\\nprint(D_expand.size(), D_expand)',\n",
       "  'import torch\\n\\nL = 2\\nN = 3\\nA = torch.randn(L,L)\\n\\n\\nD_expand = A.expand(N, L, L) # specifies new size\\nD_repeat = A.repeat(N,1,1) # specifies number of copies\\n\\nprint(D_expand.size(), D_expand)\\nprint(D_repeat.size(), D_repeat)',\n",
       "  'vars',\n",
       "  'help(vars)',\n",
       "  'vars()'],\n",
       " 'Out': {1: tensor([[[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "           [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "           [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "           ...,\n",
       "           [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "           [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "           [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       "  \n",
       "          [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "           [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "           [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "           ...,\n",
       "           [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "           [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "           [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       "  \n",
       "          [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "           [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "           [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "           ...,\n",
       "           [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "           [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "           [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "           [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "           [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "           ...,\n",
       "           [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "           [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "           [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       "  \n",
       "          [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "           [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "           [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "           ...,\n",
       "           [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "           [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "           [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       "  \n",
       "          [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "           [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "           [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "           ...,\n",
       "           [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "           [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "           [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]]]),\n",
       "  2: tensor([[[ 1.1109, -1.0908],\n",
       "           [ 0.5226,  1.4406]],\n",
       "  \n",
       "          [[ 1.1109, -1.0908],\n",
       "           [ 0.5226,  1.4406]],\n",
       "  \n",
       "          [[ 1.1109, -1.0908],\n",
       "           [ 0.5226,  1.4406]]]),\n",
       "  5: <function vars>},\n",
       " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f4a737867f0>>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7f4a73755e80>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7f4a73755e80>,\n",
       " '_': <function vars>,\n",
       " '__': tensor([[[ 1.1109, -1.0908],\n",
       "          [ 0.5226,  1.4406]],\n",
       " \n",
       "         [[ 1.1109, -1.0908],\n",
       "          [ 0.5226,  1.4406]],\n",
       " \n",
       "         [[ 1.1109, -1.0908],\n",
       "          [ 0.5226,  1.4406]]]),\n",
       " '___': tensor([[[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "          [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "          [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "          ...,\n",
       "          [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "          [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "          [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       " \n",
       "         [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "          [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "          [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "          ...,\n",
       "          [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "          [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "          [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       " \n",
       "         [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "          [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "          [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "          ...,\n",
       "          [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "          [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "          [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "          [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "          [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "          ...,\n",
       "          [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "          [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "          [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       " \n",
       "         [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "          [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "          [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "          ...,\n",
       "          [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "          [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "          [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       " \n",
       "         [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "          [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "          [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "          ...,\n",
       "          [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "          [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "          [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]]]),\n",
       " '_i': 'help(vars)',\n",
       " '_ii': 'vars',\n",
       " '_iii': 'import torch\\n\\nL = 2\\nN = 3\\nA = torch.randn(L,L)\\n\\n\\nD_expand = A.expand(N, L, L) # specifies new size\\nD_repeat = A.repeat(N,1,1) # specifies number of copies\\n\\nprint(D_expand.size(), D_expand)\\nprint(D_repeat.size(), D_repeat)',\n",
       " '_i1': 'import torch\\n\\nL = 10\\nN = 20\\nA = torch.randn(L,L)\\nA.expand(N, L, L) # specifies new size\\nA.repeat(N,1,1) # specifies number of copies',\n",
       " 'torch': <module 'torch' from '/home/kwangho/anaconda3/envs/lecture/lib/python3.6/site-packages/torch/__init__.py'>,\n",
       " 'L': 2,\n",
       " 'N': 3,\n",
       " 'A': tensor([[0.1574, 0.8316],\n",
       "         [0.4780, 0.8731]]),\n",
       " '_1': tensor([[[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "          [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "          [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "          ...,\n",
       "          [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "          [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "          [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       " \n",
       "         [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "          [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "          [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "          ...,\n",
       "          [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "          [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "          [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       " \n",
       "         [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "          [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "          [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "          ...,\n",
       "          [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "          [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "          [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "          [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "          [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "          ...,\n",
       "          [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "          [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "          [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       " \n",
       "         [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "          [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "          [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "          ...,\n",
       "          [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "          [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "          [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]],\n",
       " \n",
       "         [[-1.2431, -0.2982, -1.5437,  ...,  1.2548,  0.0549, -0.0281],\n",
       "          [ 0.2139, -0.6118, -0.9719,  ..., -0.9681, -1.7534, -1.0238],\n",
       "          [ 0.8943,  0.9053, -0.7394,  ...,  1.1705,  0.9454,  1.0429],\n",
       "          ...,\n",
       "          [ 0.1529,  0.2829, -1.9076,  ...,  0.9759,  0.1043, -0.3348],\n",
       "          [ 0.4991, -1.0853, -0.7810,  ...,  2.0185,  1.7298, -0.1799],\n",
       "          [ 0.3286,  0.1574,  0.9982,  ..., -0.7777,  1.2676, -1.0680]]]),\n",
       " '_i2': 'import torch\\n\\nL = 2\\nN = 3\\nA = torch.randn(L,L)\\n\\n\\nA.expand(N, L, L) # specifies new size\\nA.repeat(N,1,1) # specifies number of copies',\n",
       " '_2': tensor([[[ 1.1109, -1.0908],\n",
       "          [ 0.5226,  1.4406]],\n",
       " \n",
       "         [[ 1.1109, -1.0908],\n",
       "          [ 0.5226,  1.4406]],\n",
       " \n",
       "         [[ 1.1109, -1.0908],\n",
       "          [ 0.5226,  1.4406]]]),\n",
       " '_i3': 'import torch\\n\\nL = 2\\nN = 3\\nA = torch.randn(L,L)\\n\\n\\nD_expand = A.expand(N, L, L) # specifies new size\\nD_repeat = A.repeat(N,1,1) # specifies number of copies\\n\\nprint(D_expand.size(), D_expand)',\n",
       " 'D_expand': tensor([[[0.1574, 0.8316],\n",
       "          [0.4780, 0.8731]],\n",
       " \n",
       "         [[0.1574, 0.8316],\n",
       "          [0.4780, 0.8731]],\n",
       " \n",
       "         [[0.1574, 0.8316],\n",
       "          [0.4780, 0.8731]]]),\n",
       " 'D_repeat': tensor([[[0.1574, 0.8316],\n",
       "          [0.4780, 0.8731]],\n",
       " \n",
       "         [[0.1574, 0.8316],\n",
       "          [0.4780, 0.8731]],\n",
       " \n",
       "         [[0.1574, 0.8316],\n",
       "          [0.4780, 0.8731]]]),\n",
       " '_i4': 'import torch\\n\\nL = 2\\nN = 3\\nA = torch.randn(L,L)\\n\\n\\nD_expand = A.expand(N, L, L) # specifies new size\\nD_repeat = A.repeat(N,1,1) # specifies number of copies\\n\\nprint(D_expand.size(), D_expand)\\nprint(D_repeat.size(), D_repeat)',\n",
       " '_i5': 'vars',\n",
       " '_5': <function vars>,\n",
       " '_i6': 'help(vars)',\n",
       " '_i7': 'vars()'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
